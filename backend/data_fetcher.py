# backend/data_fetcher.py
"""
OpenDigger æ•°æ®åŒæ­¥æ¨¡å—
è´Ÿè´£ä» OpenDigger API æ‹‰å–æ•°æ®å¹¶å­˜å…¥æœ¬åœ°æ•°æ®åº“
"""
import json
import shutil
from pathlib import Path
import requests
import warnings
warnings.filterwarnings('ignore', message='Unverified HTTPS request')
import time
from extensions import db
from models import MetricSeries
from flask import Flask
from datetime import datetime
import json as pyjson
from contextlib import nullcontext

# âœ… å¯¼å…¥ç»Ÿä¸€çš„å·¥å…·å‡½æ•°
from metric_utils import mean as _mean, std_population as _std_pop, tail_n_values, calculate_health_score

BACKEND_ROOT = Path(__file__).parent
CONFIG_FILE = BACKEND_ROOT / "config.json"
DATA_DIR = BACKEND_ROOT / "data"
if not DATA_DIR.exists():
    DATA_DIR.mkdir(exist_ok=True)


# âœ… è¾…åŠ©å‡½æ•°ï¼šå…¼å®¹æ—§ä»£ç 
def _tail12(records):
    """è·å–æœ€è¿‘12ä¸ªæœˆæ•°æ®çš„åŒ…è£…å‡½æ•°"""
    return tail_n_values(records, n=12, month_key="month", value_key="count")

def generate_llm_summary_db():
    with ensure_app_context():
        print("--- [SUMMARY] å¼€å§‹ç”Ÿæˆ LLM ç”Ÿæ€æ±‡æ€»ï¼ˆDBç‰ˆï¼‰ ---")

        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            config = json.load(f)
        repos = config.get("repositories", [])

        summary_items = []

        for repo_info in repos:
            platform = repo_info["platform"]
            org = repo_info["org"]
            repo = repo_info["repo"]
            category = repo_info.get("category", "unknown")
            repo_key = f"{platform}/{org}/{repo}"

            # ä» DB å– openrank / activity
            row_or = MetricSeries.query.filter_by(platform=platform, entity=org, repo=repo, metric="openrank").first()
            row_act = MetricSeries.query.filter_by(platform=platform, entity=org, repo=repo, metric="activity").first()

            if not row_or or not row_act:
                print(f"--- [WARN] ç•¥è¿‡ {repo_key}ï¼Œopenrank æˆ– activity DB æ•°æ®ç¼ºå¤± ---")
                continue

            try:
                or_records = pyjson.loads(row_or.data_json or "[]")
                act_records = pyjson.loads(row_act.data_json or "[]")

                or_vals = _tail12(or_records)
                act_vals = _tail12(act_records)

                if not or_vals or not act_vals:
                    print(f"--- [WARN] ç•¥è¿‡ {repo_key}ï¼Œè¿‘12æœˆæ•°æ®ä¸è¶³ ---")
                    continue

                openrank_mean_12m = _mean(or_vals)
                openrank_std_12m = _std_pop(or_vals)
                activity_mean_12m = _mean(act_vals)

                summary_items.append({
                    "platform": platform,
                    "org": org,
                    "repo": repo,
                    "project_key": repo_key,
                    "category": category,
                    "openrank_mean_12m": float(openrank_mean_12m),
                    "openrank_std_12m": float(openrank_std_12m),
                    "activity_mean_12m": float(activity_mean_12m),
                })

            except Exception as e:
                print(f"--- [WARN] å¤„ç† {repo_key} æ±‡æ€»å¤±è´¥: {e} ---")

        if not summary_items:
            print("--- [SUMMARY] æ²¡æœ‰å¯ç”¨é¡¹ç›®ç”Ÿæˆæ±‡æ€» ---")
            return

        max_or = max(i["openrank_mean_12m"] for i in summary_items) or 1.0
        max_act = max(i["activity_mean_12m"] for i in summary_items) or 1.0
        max_std = max(i["openrank_std_12m"] for i in summary_items) or 1.0

        for item in summary_items:
            or_norm = item["openrank_mean_12m"] / max_or if max_or > 0 else 0.0
            act_norm = item["activity_mean_12m"] / max_act if max_act > 0 else 0.0
            std_norm = item["openrank_std_12m"] / max_std if max_std > 0 else 0.0
            
            # âœ… ç¨³å®šæ€§ = 1 - æ³¢åŠ¨æ€§å½’ä¸€åŒ–å€¼
            stability_norm = 1.0 - std_norm

            # âœ… ä½¿ç”¨ç»Ÿä¸€çš„å¥åº·åº¦è®¡ç®—å‡½æ•°
            item["health_score"] = calculate_health_score(or_norm, act_norm, stability_norm)
            
            item["openrank_mean_12m"] = round(item["openrank_mean_12m"], 2)
            item["activity_mean_12m"] = round(item["activity_mean_12m"], 2)
            item["openrank_std_12m"] = round(item["openrank_std_12m"], 2)

        summary_file = DATA_DIR / "llm_summary.json"
        with open(summary_file, "w", encoding="utf-8") as f:
            json.dump(summary_items, f, ensure_ascii=False, indent=2)

        print(f"--- [SUMMARY] å·²ç”Ÿæˆ LLM ç”Ÿæ€æ±‡æ€»: {summary_file} ---")


def ensure_app_context():
    try:
        from flask import current_app
        _ = current_app.name
        return nullcontext()
    except Exception:
        app = Flask("data_fetcher")
        db_path = BACKEND_ROOT / "openrank.db"
        app.config["SQLALCHEMY_DATABASE_URI"] = f"sqlite:///{db_path}"
        app.config["SQLALCHEMY_TRACK_MODIFICATIONS"] = False
        db.init_app(app)
        return app.app_context()
    
def auto_cleanup_repos(config, invalid_repos):
    """
    è‡ªåŠ¨ä» config.json ç§»é™¤æ— æ•ˆé¡¹ç›®
    åœ¨æ•°æ®åŒæ­¥æ—¶è‡ªåŠ¨è°ƒç”¨
    """
    if not invalid_repos:
        return
    
    print(f"\n--- [CLEANUP] å¼€å§‹è‡ªåŠ¨æ¸…ç† {len(invalid_repos)} ä¸ªæ— æ•ˆé¡¹ç›® ---")
    
    # æ„å»ºæ— æ•ˆé¡¹ç›®çš„ key é›†åˆ
    invalid_keys = {f"{r['org']}/{r['repo']}" for r in invalid_repos}
    
    # è¿‡æ»¤å‡ºæœ‰æ•ˆé¡¹ç›®
    valid_repos = [
        r for r in config["repositories"]
        if f"{r['org']}/{r['repo']}" not in invalid_keys
    ]
    
    # å¤‡ä»½åŸé…ç½®
    backup_file = CONFIG_FILE.with_suffix(".json.bak")
    shutil.copy(CONFIG_FILE, backup_file)
    print(f"ğŸ“¦ å·²å¤‡ä»½åŸé…ç½®: {backup_file}")
    
    # æ›´æ–°é…ç½®
    config["repositories"] = valid_repos
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ä» config.json ç§»é™¤ {len(invalid_repos)} ä¸ªæ— æ•ˆé¡¹ç›®:")
    for r in invalid_repos:
        print(f"   - {r['org']}/{r['repo']}")
    
    # æ¸…ç†æ•°æ®åº“æ®‹ç•™
    try:
        with ensure_app_context():
            deleted = 0
            for repo_info in invalid_repos:
                rows = MetricSeries.query.filter_by(
                    platform=repo_info["platform"],
                    entity=repo_info["org"],
                    repo=repo_info["repo"]
                ).all()
                for row in rows:
                    db.session.delete(row)
                    deleted += 1
            db.session.commit()
            if deleted:
                print(f"ğŸ—‘ï¸  å·²æ¸…ç†æ•°æ®åº“ {deleted} æ¡æ®‹ç•™è®°å½•")
    except Exception as e:
        print(f"âš ï¸  æ•°æ®åº“æ¸…ç†è·³è¿‡: {e}")
    
    print("--- [CLEANUP] æ¸…ç†å®Œæˆ ---\n")

def sync_opendigger_data():
    print("--- [FETCH] å¼€å§‹åŒæ­¥OpenDiggeræ•°æ® ---")

    with ensure_app_context():
        db.create_all()

        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                config = json.load(f)
            repos = config["repositories"]
            metrics = config["metrics"]
            base_url = config["data_source"]["base_url"]
        except Exception as e:
            print(f"--- [é”™è¯¯] è¯»å–é…ç½®å¤±è´¥: {e} ---")
            return

        # è®°å½•æ¯ä¸ªé¡¹ç›®çš„å¤±è´¥æŒ‡æ ‡
        repo_failures = {}  # key: "org/repo", value: set of failed metrics
        core_metrics = {"openrank", "activity"}  # æ ¸å¿ƒæŒ‡æ ‡ï¼Œå…¨éƒ¨å¤±è´¥æ‰ç®—æ— æ•ˆ

        for repo_info in repos:
            platform, org, repo = repo_info["platform"], repo_info["org"], repo_info["repo"]
            repo_key = f"{org}/{repo}"
            repo_failures[repo_key] = set()

            for metric in metrics:
                api_url = base_url.format(platform=platform, org=org, repo=repo, metric=metric)

                try:
                    resp = requests.get(api_url, timeout=30, verify=False)
                    resp.raise_for_status()
                    data = resp.json()

                    if not isinstance(data, dict):
                        raise ValueError("æ•°æ®æ ¼å¼éé”®å€¼å¯¹(dict)")

                    formatted_data = [
                        {"month": k, "count": v}
                        for k, v in data.items()
                        if isinstance(k, str) and len(k) == 7 and "-" in k
                    ]
                    if not formatted_data:
                        raise ValueError("æ— æœ‰æ•ˆæ—¶é—´æ•°æ®")

                    row = MetricSeries.query.filter_by(
                        platform=platform, entity=org, repo=repo, metric=metric
                    ).first()

                    payload = pyjson.dumps(formatted_data, ensure_ascii=False)

                    if row:
                        row.data_json = payload
                    else:
                        db.session.add(MetricSeries(
                            platform=platform, entity=org, repo=repo,
                            metric=metric, data_json=payload
                        ))

                    db.session.commit()
                    print(f"âœ… æˆåŠŸå†™å…¥DB: {platform}/{org}/{repo} - {metric}")

                except requests.HTTPError as e:
                    code = getattr(e.response, "status_code", None)
                    if code == 404:
                        print(f"âŒ è·³è¿‡ (404): {platform}/{org}/{repo} - {metric}")
                        repo_failures[repo_key].add(metric)
                    else:
                        print(f"âŒ HTTPé”™è¯¯: {platform}/{org}/{repo} - {metric} -> {e}")
                except Exception as e:
                    print(f"âŒ å¤„ç†å¤±è´¥: {platform}/{org}/{repo} - {metric} -> {e}")

        print("--- [FETCH] æ•°æ®åŒæ­¥å®Œæˆ ---")

        # === è‡ªåŠ¨æ¸…ç†æ— æ•ˆé¡¹ç›® ===
        invalid_repos = []
        for repo_info in repos:
            repo_key = f"{repo_info['org']}/{repo_info['repo']}"
            failed_metrics = repo_failures.get(repo_key, set())
            
            # æ ¸å¿ƒæŒ‡æ ‡å…¨éƒ¨å¤±è´¥ â†’ æ— æ•ˆé¡¹ç›®
            if core_metrics.issubset(failed_metrics):
                invalid_repos.append(repo_info)
                print(f"âš ï¸  æ£€æµ‹åˆ°æ— æ•ˆé¡¹ç›®: {repo_key}")

        if invalid_repos:
            auto_cleanup_repos(config, invalid_repos)

        # åŒæ­¥å®Œåç”Ÿæˆæ±‡æ€»ï¼ˆä»DBè¯»ï¼‰
        try:
            generate_llm_summary_db()
        except Exception as e:
            print(f"--- [WARN] ç”Ÿæˆ LLM ç”Ÿæ€æ±‡æ€»å¤±è´¥: {e} ---")

def should_sync(ttl_hours: int = 24) -> bool:
    summary_file = DATA_DIR / "llm_summary.json"

    # DBä¸ºç©ºæˆ–è¡¨ä¸å­˜åœ¨ä¹Ÿè¦åŒæ­¥
    with ensure_app_context():
        try:
            db.create_all()
            if MetricSeries.query.first() is None:
                return True
        except Exception:
            return True

    if not summary_file.exists():
        return True

    age_seconds = time.time() - summary_file.stat().st_mtime
    return age_seconds > ttl_hours * 3600


def run_sync(force: bool = False, ttl_hours: int = 24):
    """ç»™ Flask å¯åŠ¨æ—¶è°ƒç”¨çš„å…¥å£"""
    if not force and not should_sync(ttl_hours=ttl_hours):
        print(f"--- [FETCH] è·³è¿‡åŒæ­¥ï¼šllm_summary.json åœ¨ {ttl_hours}h å†…å·²æ›´æ–° ---")
        return
    sync_opendigger_data()
if __name__ == "__main__":
    run_sync(force=True)  # æ‰‹åŠ¨è¿è¡Œæ—¶å¼ºåˆ¶å…¨é‡åŒæ­¥
#!/usr/bin/env python3
"""
æ£€æµ‹ config.json ä¸­å“ªäº›é¡¹ç›®åœ¨ OpenDigger æ²¡æœ‰æ•°æ®
ç”¨äºæ•°æ®æ²»ç†ï¼šæ‰¾å‡ºæ— æ•ˆé¡¹ç›®å¹¶å¯é€‰æ‹©æ¸…ç†

ä½¿ç”¨æ–¹æ³•ï¼š
    python check_repos.py          # æ£€æµ‹å¹¶è¯¢é—®æ˜¯å¦æ¸…ç†
    python check_repos.py --check  # ä»…æ£€æµ‹ï¼Œä¸æ¸…ç†
"""

import json
import requests
import shutil
from pathlib import Path

# é…ç½®
CONFIG_FILE = Path(__file__).parent / "config.json"
BASE_URL = "https://oss.open-digger.cn/{platform}/{org}/{repo}/{metric}.json"
REQUIRED_METRICS = ["openrank", "activity"]  # å¿…é¡»æœ‰çš„æ ¸å¿ƒæŒ‡æ ‡


def check_repo_availability(platform, org, repo):
    """æ£€æŸ¥ä¸€ä¸ªé¡¹ç›®æ˜¯å¦æœ‰å¯ç”¨æ•°æ®"""
    missing_metrics = []
    
    for metric in REQUIRED_METRICS:
        url = BASE_URL.format(platform=platform, org=org, repo=repo, metric=metric)
        try:
            resp = requests.get(url, timeout=10)
            if resp.status_code == 404:
                missing_metrics.append(metric)
        except Exception as e:
            missing_metrics.append(f"{metric}(error)")
    
    return missing_metrics


def cleanup_database(invalid_repos):
    """æ¸…ç†æ•°æ®åº“ä¸­çš„æ— æ•ˆé¡¹ç›®æ•°æ®"""
    from flask import Flask
    from extensions import db
    from models import MetricSeries
    
    app = Flask("cleanup")
    db_path = Path(__file__).parent / "openrank.db"
    app.config["SQLALCHEMY_DATABASE_URI"] = f"sqlite:///{db_path}"
    app.config["SQLALCHEMY_TRACK_MODIFICATIONS"] = False
    db.init_app(app)
    
    with app.app_context():
        deleted_count = 0
        for repo_info in invalid_repos:
            platform = repo_info["platform"]
            org = repo_info["org"]
            repo = repo_info["repo"]
            
            # åˆ é™¤è¯¥é¡¹ç›®çš„æ‰€æœ‰æŒ‡æ ‡æ•°æ®
            rows = MetricSeries.query.filter_by(
                platform=platform, 
                entity=org, 
                repo=repo
            ).all()
            
            for row in rows:
                db.session.delete(row)
                deleted_count += 1
        
        db.session.commit()
        print(f"ğŸ—‘ï¸  å·²ä»æ•°æ®åº“åˆ é™¤ {deleted_count} æ¡è®°å½•")


def cleanup_invalid_repos(invalid_repos, valid_repos):
    """
    ä» config.json ä¸­ç§»é™¤æ— æ•ˆé¡¹ç›®
    åŒæ—¶æ¸…ç†æ•°æ®åº“ä¸­çš„æ®‹ç•™æ•°æ®
    """
    if not invalid_repos:
        print("âœ¨ æ²¡æœ‰éœ€è¦æ¸…ç†çš„é¡¹ç›®")
        return
    
    # 1. æ›´æ–° config.json
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        config = json.load(f)
    
    # ç”¨æœ‰æ•ˆé¡¹ç›®åˆ—è¡¨æ›¿æ¢
    config["repositories"] = valid_repos
    
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_file = CONFIG_FILE.with_suffix(".json.bak")
    shutil.copy(CONFIG_FILE, backup_file)
    print(f"ğŸ“¦ å·²å¤‡ä»½åŸé…ç½®åˆ°: {backup_file}")
    
    # å†™å…¥æ–°é…ç½®
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… config.json å·²æ›´æ–°ï¼Œç§»é™¤äº† {len(invalid_repos)} ä¸ªæ— æ•ˆé¡¹ç›®")
    
    # 2. æ¸…ç†æ•°æ®åº“ï¼ˆå¯é€‰ï¼‰
    try:
        cleanup_database(invalid_repos)
    except Exception as e:
        print(f"âš ï¸  æ•°æ®åº“æ¸…ç†è·³è¿‡ï¼ˆå¯èƒ½ç¼ºå°‘ä¾èµ–ï¼‰: {e}")


def main(check_only=False, auto_clean=False):
    """
    ä¸»å‡½æ•°
    Args:
        check_only: æ˜¯å¦ä»…æ£€æµ‹ä¸æ¸…ç†
    """
    # è¯»å–é…ç½®
    with open(CONFIG_FILE, "r", encoding="utf-8") as f:
        config = json.load(f)
    
    repos = config.get("repositories", [])
    
    print("=" * 60)
    print("ğŸ” å¼€å§‹æ£€æµ‹é¡¹ç›®æ•°æ®å¯ç”¨æ€§...")
    print(f"   å…± {len(repos)} ä¸ªé¡¹ç›®å¾…æ£€æµ‹")
    print("=" * 60)
    
    invalid_repos = []
    valid_repos = []
    
    for i, repo_info in enumerate(repos, 1):
        platform = repo_info["platform"]
        org = repo_info["org"]
        repo = repo_info["repo"]
        full_name = f"{org}/{repo}"
        
        print(f"[{i}/{len(repos)}] æ£€æµ‹ {full_name}...", end=" ")
        
        missing = check_repo_availability(platform, org, repo)
        
        if missing:
            print(f"âŒ ç¼ºå¤±: {missing}")
            invalid_repos.append(repo_info)
        else:
            print("âœ…")
            valid_repos.append(repo_info)
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 60)
    print("ğŸ“Š æ£€æµ‹ç»“æœæ±‡æ€»")
    print("=" * 60)
    print(f"   æœ‰æ•ˆé¡¹ç›®: {len(valid_repos)} ä¸ª")
    print(f"   æ— æ•ˆé¡¹ç›®: {len(invalid_repos)} ä¸ª")
    
    if invalid_repos:
        print("\nâš ï¸  ä»¥ä¸‹é¡¹ç›®åœ¨ OpenDigger æ— æ•°æ®ï¼Œå»ºè®®åˆ é™¤ï¼š")
        print("-" * 40)
        for r in invalid_repos:
            print(f"   â€¢ {r['org']}/{r['repo']} ({r.get('category', 'unknown')})")
        
        if not check_only:
            if auto_clean:
                # è‡ªåŠ¨æ¸…ç†æ¨¡å¼
                print("\nğŸ¤– è‡ªåŠ¨æ¸…ç†æ¨¡å¼ï¼Œå¼€å§‹æ¸…ç†...")
                cleanup_invalid_repos(invalid_repos, valid_repos)
                print("\nğŸ‰ æ¸…ç†å®Œæˆï¼")
            else:
                # äº¤äº’ç¡®è®¤
                print("\n" + "-" * 60)
                confirm = input("æ˜¯å¦ç«‹å³æ¸…ç†è¿™äº›æ— æ•ˆé¡¹ç›®ï¼Ÿ(y/n): ").strip().lower()
                if confirm == 'y':
                    cleanup_invalid_repos(invalid_repos, valid_repos)
                    print("\nğŸ‰ æ¸…ç†å®Œæˆï¼è¯·é‡æ–°è¿è¡Œ data_fetcher.py æ›´æ–°æ•°æ®")
                else:
                    print("å·²å–æ¶ˆæ¸…ç†æ“ä½œ")
    else:
        print("\nğŸ‰ æ‰€æœ‰é¡¹ç›®æ•°æ®éƒ½å¯ç”¨ï¼Œæ— éœ€æ¸…ç†ï¼")
    
    return invalid_repos, valid_repos


if __name__ == "__main__":
    import sys
    
    check_only = "--check" in sys.argv
    auto_clean = "--auto" in sys.argv
    
    if check_only:
        print("ğŸ“‹ ä»…æ£€æµ‹æ¨¡å¼ï¼ˆä¸ä¼šä¿®æ”¹ä»»ä½•æ–‡ä»¶ï¼‰\n")
    elif auto_clean:
        print("ğŸ¤– è‡ªåŠ¨æ¸…ç†æ¨¡å¼ï¼ˆæ— éœ€ç¡®è®¤ï¼‰\n")
    
    main(check_only=check_only, auto_clean=auto_clean)